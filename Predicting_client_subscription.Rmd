---
title: "Predicting_client_subscription"
author: "Felix Seo"
date: '2022-07-04'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
library(caret)
library(mlbench)
library(tidyverse)
library(corrplot)
library(rcompanion)
library(glmnet)
library(gglasso)
library(rpart)
library(rpart.plot)
```

# ideas for better prediction

add a column with information if it is the first or second phone call as the same customer often was contacted twice to check if term deposit. 

## Data preparation

Starting with looking at the data, the following attribute information follows [Moro et al., 2014].

### input variables:
bank client data:

* 1 - age (numeric)

* 2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')

* 3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)

* 4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')

* 5 - default: has credit in default? (categorical: 'no','yes','unknown')

* 6 - housing: has housing loan? (categorical: 'no','yes','unknown')

* 7 - loan: has personal loan? (categorical: 'no','yes','unknown')

related with the last contact of the current campaign:

* 8 - contact: contact communication type (categorical: 'cellular','telephone')

* 9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')

* 10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')

* 11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.

other attributes:

* 12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)

* 13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)

* 14 - previous: number of contacts performed before this campaign and for this client (numeric)

* 15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')

social and economic context attributes

* 16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)

* 17 - cons.price.idx: consumer price index - monthly indicator (numeric)

* 18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)

* 19 - euribor3m: euribor 3 month rate - daily indicator (numeric)

* 20 - nr.employed: number of employees - quarterly indicator (numeric)

Output variable (desired target):

* 21 - y - has the client subscribed a term deposit? (binary: 'yes','no')

Seen in the attribute information the 11th input variable is not desired for use in an predictive model, why it is removed. The default attribute is also removed since it only has 3 realizations of people with credit in default. This huge difference in group size will not contribute to the predicative power of the model. Attributes 2 to 7 all have the risk of containing missing values labeled "unknown". The data containing any of the missing values in attributes 2 to 7 are removed, which ended up being roughly 3000 instances (still the dataset contains approximately 38000 instances). For the categorical variables the appropriate encoding is conducted. The binary variables are transformed to 0 and 1 outcomes. The ordinal or nominal category classification for the variables are debatable. The education variable is easily classified as an ordinal type variable why we will use simple ordinal encoding. Variables like marital status, last contact day of the week and month are classified as nominal variables and one-hot encoding is conducted. The latter is arguably nominal in this context as the month seems not to have an natural ordering for this classification task.      


```{r}
bank_data <- read_delim("Data/bank_additional_full.csv", delim = ";")
```


```{r}
#trying to get data on nice format, one-hot encoding. 
bank_data_mod <- bank_data %>% 
  dplyr::select(-duration, -default) %>% 
  mutate(
    temp1 = 1, 
    temp2 = 1,
    temp3 = 1, 
    temp4 = 1,
    temp5 = 1,
    id_col = seq(1, length(bank_data$job), 1)
  ) %>% #id_col because pivot easy to work with then, removed later
  filter(if_all(job:loan, ~ . != "unknown")) %>% 
  pivot_wider(names_from = job, values_from = temp1, values_fill = 0) %>% 
  pivot_wider(names_from = marital, values_from = temp2, values_fill = 0) %>% 
  pivot_wider(names_from = month, values_from = temp3, values_fill = 0) %>%
  pivot_wider(names_from = day_of_week, values_from = temp4, values_fill = 0) %>% 
  pivot_wider(names_from = poutcome, values_from = temp5, values_fill = 0) %>% 
  mutate(
    education = str_replace(education, "illiterate", "0"),
    education = str_replace(education, "basic.4y", "1"),
    education = str_replace(education, "basic.6y", "2"),
    education = str_replace(education, "basic.9y", "3"),
    education = str_replace(education, "high.school", "4"),
    education = str_replace(education, "professional.course", "5"),
    education = str_replace(education, "university.degree", "6"),
    education = as.numeric(education)
  ) %>%
  mutate(
    housing = if_else(housing == "yes", 1, 0),
    loan = if_else(loan == "yes", 1, 0),
    contact = if_else(contact == "cellular", 1, 0), #0 means telephone, 1 cellular.
    y = if_else(y == "yes", 1, 0)
  ) %>% 
  rename(
    "blue.collar" = "blue-collar",  #donÂ´t like variables with -. 
    "self.employed" = "self-employed"
  ) %>% 
  select(-id_col) %>% 
  slice_sample(prop = 1) # randomize the data.

bank_data_mod
```

```{r, eval=FALSE}
train_data <- bank_data_mod %>% slice_head(prop = 0.8) %>% select(-student, -thu)
test_data <- bank_data_mod[length(train_data$y):length(bank_data_mod$y),]  %>% select(-student, -thu)

model1 <- glm(y ~ ., data = train_data, family = "binomial")

#model2 <- glm(y ~ 1, data = bank_data_mod, family = "binomial")

predicted <- predict(model1, newdata = test_data, type = "response")
predicted <- ifelse(predicted > 0.5,1,0) %>% as.vector() %>% as.factor()
#sum(results) / length(results)
confusionMatrix(predicted, as.factor(test_data$y))


#test <- glmnet(model.matrix(y ~ ., bank_data_mod), bank_data_mod$y, family = "binomial", alpha = 1, lambda = NULL)
#cv.lasso <- cv.glmnet(model.matrix(y ~ ., bank_data_mod), bank_data_mod$y, family = "binomial", alpha = 1)

#stepAIC(model2, scope=list(upper=model1,lower=model2), direction="both")
```

```{r}
#The gglasso package. group lasso method. I try under sampling the data
set.seed(990108)

bank_data_glasso <- bank_data_mod %>% 
  select(-y) %>% 
  scale() %>% #standardized data for lasso.
  as.data.frame() %>% 
  add_column(y = bank_data_mod$y)
  
groups = c(seq(1, 13), rep(14, 11), rep(15, 3), rep(16, 10), rep(17, 5), rep(18, 3))

success_y <- bank_data_glasso %>% 
  filter(y == 1)

data_under_samp <- bank_data_glasso %>% 
  filter(y == 0) %>% 
  slice_sample(n = length(success_y$y)) %>% 
  add_row(success_y) %>% 
  slice_sample(prop = 1) #randomize the rows, before the where ordered
  
train_under_samp <- data_under_samp %>% 
  slice_head(prop = 0.8) #rows already randomized 
  
test_under_samp <- data_under_samp %>% 
  slice_tail(prop = 0.2)


#########
#data_glasso_y <- bank_data_mod %>% 
#  select(y) %>%
#  slice_head(n = 1000) %>%   
#  mutate(y = if_else(y == 0, -1, y)) %>% #gglasso wants -1 1 responses only
#  as.matrix()
  
#data_glasso_pred <- bank_data_mod %>%
#  select(-y) %>%
#  scale() %>% 
#  {.[1:1000, ]} 
###############

train_glasso_pred <- train_under_samp %>% 
  select(-y) %>% 
  as.matrix()

train_glasso_y <- train_under_samp %>% 
  select(y) %>% 
  mutate(y = if_else(y == 0, -1, y)) %>% #gglasso wants -1/1 responses only
  as.matrix()
    
test_glasso_pred <- test_under_samp %>% 
  select(-y) %>% 
  scale() 

cv_glasso <- cv.gglasso(
  train_glasso_pred,
  train_glasso_y,
  group = groups,
  loss = "logit",
  lambda = c(seq(0.00001, 0.001, 0.00001), 0.002, 0.003, 0.004),
  pred.loss = "loss",
  nfolds = 10
)
plot(cv_glasso)

model_glasso <- gglasso(
  train_glasso_pred, #only predictors chosen
  train_glasso_y,
  group = groups,
  loss = "logit",
  lambda = cv_glasso$lambda.min,
)
  
test <- model_glasso %>% 
  predict(newx = test_glasso_pred, type = "link") %>% 
  as.data.frame() %>% 
  rename("pred" = "s0") %>% 
  mutate(y = test_under_samp$y, pred = 1 / (1 + 2.72^-pred)) #%>% 
  #{verification::roc.plot(.$y, .$pred, thresholds = seq(0, 1, 0.01))} 


#test_pred %>% 
 # as.data.frame() %>%
 # select(s2) %>% 
#  mutate(s2 = 1 / (1 + 2.72^(-s2)), s2 = if_else(s2 > 0.4, 1, 0), s2 = as.factor(s2))  
#

#confusionMatrix(hej$s2, as.factor(test_under_samp$y), positive = "1")

```

```{r}
set.seed(990108)
#classification trees 

#Fixing the nominall data to be ordered proportions
bank_data_tree <- bank_data %>% 
  select(-duration, -default)  %>% 
  filter(if_all(job:y, ~ . != "unknown")) %>% 
  mutate(
    education = str_replace(education, "illiterate", "0"),
    education = str_replace(education, "basic.4y", "1"),
    education = str_replace(education, "basic.6y", "2"),
    education = str_replace(education, "basic.9y", "3"),
    education = str_replace(education, "high.school", "4"),
    education = str_replace(education, "professional.course", "5"),
    education = str_replace(education, "university.degree", "6"),
    education = as.numeric(education)
  ) %>%
  mutate(
    housing = if_else(housing == "yes", 1, 0),
    loan = if_else(loan == "yes", 1, 0),
    contact = if_else(contact == "cellular", 1, 0),
    y = if_else(y == "yes", 1, 0),
    poutcome = if_else(poutcome == "success", 1, 0)
  ) 

data_tree_success <- bank_data_tree %>% 
  filter(y == 1)
  
data_tree_balanced <- bank_data_tree %>% 
  filter(y == 0) %>% 
  slice_sample(n = length(data_tree_success$y)) %>% 
  add_row(data_tree_success) %>% 
  group_by(job) %>% 
  mutate(job = sum(y) / length(data_tree_success$y)) %>% # the prop of successes in each job, mean works since 1/0 outcome
  ungroup() %>%
  group_by(marital) %>% 
  mutate(marital = sum(y) / length(data_tree_success$y)) %>% # the prop of successes in each job, mean works since 1/0 outcome
  ungroup() %>%
  group_by(month) %>% 
  mutate(month = sum(y) / length(data_tree_success$y)) %>% # the prop of successes in each job, mean works since 1/0 outcome
  ungroup() %>%
  group_by(day_of_week) %>% 
  mutate(day_of_week = sum(y) / length(data_tree_success$y)) %>% # the prop of successes in each job, mean works since 1/0 outcome
  ungroup() %>%  
  slice_sample(prop = 1) %>% #randomize
  mutate(month = round(month, 2), month = as.factor(month))
#data_tree_balanced #%>% distinct(prop) %>% select(prop) #%>% {sum(.$prop)}


train_tree <- data_tree_balanced %>% slice_head(n = 0.8*length(data_tree_balanced$y))
test_tree <- data_tree_balanced %>% slice_tail(n = 0.2*length(data_tree_balanced$y))

tree_parameters <- rpart.control(minsplit = 60, maxcompete = 10, maxdepth = 5, cp = 0.01)
tree_model <- rpart(y ~ ., data = train_tree, method = "class", control = tree_parameters)
rpart.plot(tree_model, extra = 106)


test2 <- predict(tree_model, test_tree, type = "prob") 
rownames(test2) <- NULL

 #%>% 
  #{verification::roc.plot(.$y, .$yes)} 

```

```{r}

hej <- test2 %>%  
  as.data.frame() %>%
  rename("no" = "0", "yes" = "1") %>%  
  select(-no) %>% 
  add_column(test) %>% 
  select(-y)


verification::roc.plot(test$y, hej)


```



```{r,eval = FALSE}
#testing correlation 
bank_data %>% 
  dplyr::select(loan, housing) %>% 
  group_by(loan, housing) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  pivot_wider(names_from = loan, values_from = count) #%>%
  #column_to_rownames(var = "job") %>% 
  dplyr::select(-job) %>% 
  data.matrix() %>% 
  cramerV() #understand bias correction!

cram_v <- function(variable1, variable2, data, bias = TRUE){
  # input names of columns in variable1 and variable2 (nominal) to return the
  # cramerÂ´s phi value. bias = TRUE means use of bias correction.  
  data %>% 
    dplyr::select(variable1, variable2) %>% 
    group_by(across()) %>% #will not accept string input, why across is used.
    summarise(count = n()) %>% 
    ungroup() %>% 
    pivot_wider(names_from = variable1, values_from = count, values_fill = 0) %>% # if NA, then zero realizations of this combination.
    column_to_rownames(var = variable2) %>% 
    data.matrix() %>% 
    cramerV(bias.correct = bias) %>% 
    return()
}

```

```{r, eval = FALSE}
nominal_var <- c("job", "marital", "loan", "housing", "contact", "month")
comb <- combn(nominal_var, m = 2)
cramer_res <- c() #matrix(NA, nrow = length(nominal_var), ncol = length(nominal_var))


for (i in 1:length(comb[1,])) {
  res <- cram_v(comb[1, i], comb[2, i], bank_data)
  cramer_res[i] <- res
}

cram_v_matrix <- matrix(
  NA, 
  nrow = length(nominal_var), 
  ncol = length(nominal_var), 
  dimnames = list(nominal_var, nominal_var)
) 
cram_v_matrix[lower.tri(cram_v_matrix, diag=FALSE)] <- cramer_res  
cram_v_matrix <- t(cram_v_matrix)
cram_v_matrix[lower.tri(cram_v_matrix, diag=FALSE)] <- cramer_res 
diag(cram_v_matrix) <- 1
corrplot(cram_v_matrix, is.corr = FALSE)
```

```{r; eval = FALSE}
bank_data_mod %>% 
  select(
    y, 
    age, 
    education, 
    euribor3m, 
    cons.price.idx,
    nr.employed, 
    cons.conf.idx,
    campaign,
    pdays, 
    previous
  ) %>% 
  cor(method = "spearman") %>% 
  corrplot(type = "upper")
```




```{r, eval = FALSE}
# random forest training
train_x <- bank_data_mod %>%
  select(-y)
  #mutate(id_col = 1:length(bank_data_mod$housing)) %>% 
  #slice_sample(prop = 0.8) 

train_y <- bank_data_mod%>% 
  select(y) 

test_x <- bank_data_mod %>%
  mutate(id_col = 1:length(bank_data_mod$housing)) %>%   
  anti_join(train_x, by = "id_col") 

test_y <- test_x %>% 
  select(y) %>% 
  mutate(y = as_factor(y))

train_x <- train_x %>% 
  select(-id_col, -y)

test_x <- test_x %>% 
  select(-id_col, -y)

test <- bank_data_mod %>% 
  mutate(y = as.factor(y)) 
  
forest <- randomForest(
  y ~ ., 
  data = test, 
  ntree = 500, 
  sampsize = 0.4 * nrow(bank_data_mod),
  nodesize = 2
)

forest
```



```{r, eval = FALSE}
# recursive feature eliminaton 
control <- rfeControl(functions = rfFuncs, method="cv", number=10)

results <- bank_data_mod %>% 
  select(-y, -euribor3m, -previous) %>%
  head(n = 1000) %>% 
  rfe(
    as.factor(bank_data_mod$y[1:1000]), 
    sizes = c(1:10),
    metric = "Accuracy", 
    rfeControl = control
  )

print(results)
plot(results, type=c("g", "o"))
#sum(predict(results, bank_data_mod[1001:2000,])$pred == bank_data_mod$y[1001:2000])   


```




